---
title: "data processing 2"
format: html
editor: visual
---

## Task

Adding another batch of files.

## Greek

task: speaker 1-4, add all emotions' 17 sentences (bc 10, 12, 14 already exist)

```{r}
# ==== 0) SET YOUR PATHS ====
source_dir <- "C:/Users/yuka/Desktop/QP1/AESDD"
target_dir <- "C:/Users/yuka/Desktop/linguist245B_project/experiment/stim/additional_items_temploc"


# ==== 1) CONFIGURATION ====
# which sub-folders to use:
emotion_folders <- list(
  a = "anger",
  f = "fear",
  h = "happiness",
  s = "sadness"
)
# corresponding prefixes:
emotion_prefix <- list(
  a = "angry__",
  f = "fear__",
  h = "happy__",
  s = "sad__"
)

# sentences to skip, speakers to keep
exclude_sentences <- c(10, 12, 14)
include_speakers  <- 1:4

# ==== 2) PROCESS ====
for (letter in names(emotion_folders)) {
  folder_path <- file.path(source_dir, emotion_folders[[letter]])
  if (!dir.exists(folder_path)) next

  # list everything, then pick only .wav
  all_files <- list.files(folder_path, full.names = TRUE)
  wavs <- all_files[tolower(tools::file_ext(all_files)) == "wav"]

  for (wav in wavs) {
    fname      <- basename(wav)                        # "s05 (2).wav"
    no_ext     <- tools::file_path_sans_ext(fname)     # "s05 (2)"
    n          <- nchar(no_ext)

    # must be exactly 7 chars: 1 + 2 digits + " " + "(" + digit + ")"
    if (n != 7)                  next
    if (substr(no_ext, 1, 1)     != letter) next       # right emotion code
    if (substr(no_ext, 4, 4)     != " ")    next       # space
    if (substr(no_ext, 5, 5)     != "(")    next
    if (substr(no_ext, 7, 7)     != ")")    next

    # parse sentence: chars 2–3
    sent_num    <- as.integer(substr(no_ext, 2, 3))
    if (is.na(sent_num) || sent_num %in% exclude_sentences) next

    # parse speaker: char 6
    speaker_num <- as.integer(substr(no_ext, 6, 6))
    if (is.na(speaker_num) || !(speaker_num %in% include_speakers)) next

    # build new name & copy
    new_name <- paste0("greek_", emotion_prefix[[letter]], fname)
    file.copy(wav,
              file.path(target_dir, new_name),
              overwrite = TRUE)
  }
}

```

## Canadian French

task: add speaker 3, 5, 8, 10’s sentences 1, 5, 6 (cus currently only has 2 3 4)

```{r}
#here is the idea
# 0) set your source + target directories
source_dir <- "C:/Users/yuka/Desktop/QP1/CaFE_48k"
target_dir <- "C:/Users/yuka/Desktop/linguist245B_project/experiment/stim/additional_items_temploc"

# 1) which emotion folders (in French), and the subfolder we want
emotion_folders <- c("Colère", "Joie", "Neutre", "Peur", "Surprise", "Tristesse")
subfolder      <- "Fort"

# 2) mapping from French‐letter to English prefix
prefix_map <- c(
  C = "angry__",    # Colere
  J = "happy__",    # Joie
  N = "neutral__",  # Neutre
  P = "fear__",     # Peur
  S = "surprise__", # Surprise
  T = "sad__"       # Tristesse
)

# 3) which speakers & sentences to keep
include_speakers <- c("03","05","08","10")
include_sentences <- c("1","5","6")

# 4) walk through each emotion
for (emo in emotion_folders) {
  emo_dir <- file.path(source_dir, emo, subfolder)
  if (!dir.exists(emo_dir)) next
  
  # list all .wav files
  wavs <- list.files(emo_dir, pattern="\\.wav$", full.names=TRUE)
  
  for (src in wavs) {
    fn <- basename(src)                 # e.g. "03-C-2-5.wav"
    parts <- strsplit(tools::file_path_sans_ext(fn), "-", fixed=TRUE)[[1]]
    # expect exactly 4 parts: speaker, letter, "2", sentence
    if (length(parts) != 4) next
    speaker <- parts[1]
    letter  <- parts[2]
    mid     <- parts[3]
    sent    <- parts[4]
    
    # filter by speaker, by the fixed "2", and by sentence
    if (!(speaker %in% include_speakers)) next
    if (mid != "2")                      next
    if (!(sent %in% include_sentences)) next
    
    # look up the right prefix
    if (!(letter %in% names(prefix_map))) next
    eng_pref <- prefix_map[letter]
    
    # build new name and copy
    new_fn <- paste0("french_", eng_pref, fn)
    file.copy(src,
              file.path(target_dir, new_fn),
              overwrite = TRUE)
  }
}

```

## Japanese

task: add alll the other regular sentences for each emotion from all speakers. (meaning, cut all of them first on praat. (here now– Praat cannot load all 720 items at one time so doing by emotion, starting from anger)

```{r}
# 0) set your source + target directories
source_root <- "C:/Users/yuka/Desktop/QP1/jvnv_ver1/jvnv_v1"
target_dir  <- "C:/Users/yuka/Desktop/linguist245B_project/experiment/stim/jp_raw2"

if (!dir.exists(target_dir)) {
  dir.create(target_dir, recursive = TRUE)
}

# 1) speakers and emotions
speakers   <- c("F1", "F2", "M1", "M2")
skip_emo   <- "disgust"
valid_em   <- c("anger", "fear", "happy", "sad", "surprise")  # folder names
prefix_map <- c(
  anger    = "angry__",
  happy    = "happy__",
  fear     = "fear__",
  surprise = "surprise__",
  sad      = "sad__"
)

# 2) which sentence numbers to keep
keep_range <- 4:40

# 3) traverse
for (spk in speakers) {
  spk_dir <- file.path(source_root, spk)
  if (!dir.exists(spk_dir)) next

  # find all emotion subfolders except "disgust"
  emos <- list.dirs(spk_dir, full.names = FALSE, recursive = FALSE)
  emos <- setdiff(emos, skip_emo)

  for (emo in emos) {
    if (!(emo %in% valid_em)) next

    reg_dir <- file.path(spk_dir, emo, "regular")
    if (!dir.exists(reg_dir)) next

    # list .wav files
    wavs <- list.files(reg_dir, pattern = "\\.wav$", full.names = TRUE)
    for (src in wavs) {
      fn <- basename(src)                             # e.g. "F1_anger_regular_04.wav"
      parts <- strsplit(tools::file_path_sans_ext(fn), "_", fixed = TRUE)[[1]]
      # expect: parts = c(speaker, emotion, "regular", sent_str)
      if (length(parts) != 4) next

      emo_tag <- parts[2]
      sent_str <- parts[4]
      sent_num <- as.integer(sent_str)                # "04" -> 4

      # filter by sentence range
      if (is.na(sent_num) || !(sent_num %in% keep_range)) next

      # rename & copy
      prefix <- prefix_map[emo_tag]
      new_fn <- paste0("japanese_", prefix, fn)
      file.copy(src,
                file.path(target_dir, new_fn),
                overwrite = TRUE)
    }
  }
}

```

After all these, normalize intensities. (again, Praat)

Then add these new files in to the experiment.html code.

Set a new rule to get a list of files so that one person sees only a set amount of sounds.

## —Results Processing—

```{r}
getwd()

```

```{r}
setwd("C:/Users/yuka/Desktop/linguist245B_project/data/temp_data")
```

```{r}
data <- read.csv("C:/Users/yuka/Downloads/wecj3vb92g.csv",
                header = TRUE, sep = ",")

View(data)
```

The goal df:

response, trial num, stim filename, language, targetEmo, score

To clean up:

-   delete col1, no rt needed

-   delete col2, no raw resp needed

-   delete col3 trial type, this info not needed (but maybe needed to sort out rows)

-   delete col4 trial index, i want trial num eventually, but can add this later once row cleaned up

-   delete col5, no time needed

-   delete col6, no internal node id needed

-   delete col7, this info can be taken from another col, maybe helpful for row clean up

-   delete col8, no infor here

-   delete, col9-13, no info here

-   keep col14. this is file ID

-   keep col15, language

-   keep col16, correct_reponse, this is the target emo

-   delete col17 eventually, helpful to separate out choice vs attention, or maybe for row cleanup

-   keep col18, this is the main resp.

```{r}


library(dplyr)
library(tibble)

prolID = data[1,2]
cleandf <- data %>%
  filter(trial_part == "choice")

cleandf <- cleandf %>% 
  select(15, 16, 18)
library(dplyr)

cleandf <- cleandf %>%
  add_column(ProlID = prolID, .before = 1)




```

# Process ALL CSVs ————

```{r}
#############################

# MAIN DATA!!!!

#############################


library(dplyr)
library(tibble)
library(purrr)

# 1) Get all CSV paths
files <- list.files(
  path       = "C:/Users/yuka/Desktop/linguist245B_project/data/osfstorage-archive",
  pattern    = "\\.csv$",
  full.names = TRUE
)

#--
# initialize
attentioncheck_failure <- c()
attentioncheck_passed<- c()
big_df <- NULL  # will grow via rbind

for (fpath in files) {
  # read data and get ProlID
  data <- read.csv(fpath, header = TRUE, sep = ",")
  prolID <- data[1, 2]

  # ---- attention‐check logic ----
  ac_rows <- grep("attentioncheck", data$stimulus)
  if (length(ac_rows) >= 3) {
    ac_corrects <- data$correct[ac_rows + 1]
    attentioncheck_passed<- c(attentioncheck_passed, prolID)
    if (any(ac_corrects == FALSE)) {
      attentioncheck_failure <- c(attentioncheck_failure, prolID)
      next
    }
  }
  # --------------------------------

  # filter to choice trials
  choice_idx <- which(data$trial_part == "choice")
  # select columns 2,14,15,16,18
  sub_data <- data[choice_idx, c(2, 14, 15, 16, 18)]
  # add ProlID as first column
  sub_data <- cbind(ProlID = prolID, sub_data)

  # append to big_df
  big_df <- if (is.null(big_df)) sub_data else rbind(big_df, sub_data)
}


#old
# 2) Process and bind them
#big_df <- files %>%
#  map_df(function(fpath) {
#    data   <- read.csv(fpath, header = TRUE, sep = ",")
#   prolID <- data[1, 2]  

#   data %>%
#      filter(trial_part == "choice") %>%
#     select(2,14,15, 16, 18) %>%
#      add_column(ProlID = prolID, .before = 1)
#  })


# big_df now has all participants’ cleaned data stacked together

####added 14 for more item info

big_df <- big_df %>%
  mutate(
    response = recode(
      response,
      `0` = "Happy",
      `1` = "Sad",
      `2` = "Angry",
      `3` = "Fear",
      `4` = "Surprise",
      `5` = "Neutral"
    )
  )


# Make sure 'correct' is numeric 0/1
big_df <- big_df %>%
  mutate(correct = as.integer(as.logical(correct)))

```

```{r}
########
# addingspeaker ID and sentence ID
########


#Add speakerID and sentenceID based on stimulus_file
big_df <- big_df %>%
  mutate(
    filename = sub("^stim/final_stim_scaled/", "", stimulus_file)
  ) %>%
  mutate(
    speakerID = case_when(
      # ── French: after “__”, take digits before the first “-”
      language == "french" ~ sub("^.*__(\\d+)-.*$", "\\1", filename),
      
      # ── Greek: take the number inside parentheses “(...)”
      language == "greek"  ~ sub("^.*\\((\\d+)\\).*",      "\\1", filename),
      
      # ── Japanese: after the first “__”, take everything up to the next “_”
      language == "japanese" ~ sub("^.*?__(.*?)_.*$",     "\\1", filename),
      
      # ── Thai: find “actor” + exactly 3 digits → that is speakerID
      language == "thai"   ~ sub("^.*actor(\\d{3}).*$",    "\\1", filename),
      
      # ── otherwise: NA
      TRUE ~ NA_character_
    )
  ) %>%
  #extract sentenceID according to language‐specific rules
  mutate(
    sentenceID = case_when(
      # ── French: take the number immediately before “_intscaled.wav”
      language == "french" ~ sub("^.*-(\\d+)_intscaled\\.wav$", "\\1", filename),
      
      # ── Greek: take the digits immediately before the space + “(”
      language == "greek"  ~ sub("^.*?([0-9]+)\\s*\\(\\d+\\).*",  "\\1", filename),
      
      # ── Japanese: take the digits immediately before “__cut_intscaled.wav”
      language == "japanese" ~ sub("^.*_(\\d+)__cut_intscaled\\.wav$", "\\1", filename),
      
      # ── Thai: find “script” + digits right after → that is sentenceID
      language == "thai"   ~ sub("^.*script(\\d+).*",       "\\1", filename),
      
      # ── otherwise: NA
      TRUE ~ NA_character_
    )
  ) %>%
  #drop the temporary filename column if you don’t need it
  select(-filename)

############ New change


big_df <- big_df %>%
  mutate(
    sentenceID = case_when(
      language == "french"   ~ paste0(sentenceID, "f"),
      language == "thai"     ~ paste0(sentenceID, "t"),
      language == "greek"    ~ paste0(sentenceID, "g"),
      language == "japanese" ~ paste0(sentenceID, "j"),
      TRUE                   ~ sentenceID
    )
  )


```

NOW DEMOGRAPHIC DATA

```{r}

library(dplyr)
library(stringr)

#read data
demo <- read.csv("C:/Users/yuka/Desktop/linguist245B_project/data/demographics_final.csv", header = TRUE, sep = ",")


# 1) pull out the raw IDs
drop_ids <- str_extract(
  datatodrop,
  '(?<="prolific_id":")[^"]+'
)

# 2) filter demo so that only needed info stays
demo_filtered <- demo %>%
  filter(
    Status == "APPROVED",
    !Participant.id %in% drop_ids
  ) %>%
  select(-c(1,4,5,6,7,8,10:21,25:28))
```

### Data drop pt1

attention check all passed!

Technical difficulity

-   I recoded this person {"prolific_id":"67c585f8e60aec06bb2e7863"} 's response as No, because i dont think they counted which question number it was, and all the other answers after were also "No" so I think it's highly likely that the 7 is a typo.

-   Only 2 real issues potentially:

    -   {"prolific_id":"65da0c93826bdadcf98372a9"}...said "a couple in and out"

    -   {"prolific_id":"683666e57971ab1ebb826601"}...said just Yes

```{r}
techdiff_IDs <- c('{"prolific_id":"65da0c93826bdadcf98372a9"}', '{"prolific_id":"683666e57971ab1ebb826601"}')
```

Unqualified Criteria

-   below are the people who self-reported to be exposed to at least one of the stimuli languages

-   12 in total

```{r}
#cleaned_surveyr
unqualifieddata<-subset(cleaned_surveyr, is.na(cleaned_surveyr$media_exposure_to_stimuli_lang) == FALSE | is.na(cleaned_surveyr$family_exposure_to_stimuli_language) == FALSE)

unqualifiedP_IDs <-unqualifieddata$ProlID
print(unqualifiedP_IDs)
```

Other issues? (noticed by manual screening)

-   {"prolific_id":"68378ae158654a68e5b29f74"}...this person answered they took 10 years of English.

-   {"prolific_id":"67c71622fd09b1bc69452143"} and {"prolific_id":"673c94098ca9d9f0d710726f"}...almost exact same answers twice.

```{r}
others_IDs <- c('{"prolific_id":"68378ae158654a68e5b29f74"}','{"prolific_id":"67c71622fd09b1bc69452143"}',  '{"prolific_id":"673c94098ca9d9f0d710726f"}')
```

If I were to get rid of all of these data, it will be -2 -12 -3 = -17 data.

119-17= 102 data overall

```{r}
#data drop from bigdf
datatodrop <- c(techdiff_IDs, unqualifiedP_IDs, others_IDs)


length(unique(big_df$ProlID))  

big_df <- big_df[!(big_df$ProlID %in% datatodrop), ]

length(unique(big_df$ProlID))  

```

## Visualizations

### Bar

```{r}
library(ggplot2)

ggplot(big_df, aes(x = correct_response, y = correct, fill = language)) +
  stat_summary(
    fun     = mean,
    geom    = "col",
    position = position_dodge(width = 0.8)
  ) +
  stat_summary(
    fun.data  = mean_cl_normal,       # mean ± 1.96*SE
    geom      = "errorbar",
    position  = position_dodge(width = 0.8),
    width     = 0.2
  ) +
  scale_fill_discrete(
    name   = "Language",
    labels = c("French", "Greek", "Japanese", "Thai")
  ) +
  labs(
    x    = "Emotion",
    y    = "Mean Accuracy",
    fill = "Language"
  ) +
  theme_minimal()

```

### Confusion matrix

**-------GO STRAIGHT TO THE LAST 2 CELLS TO PRINT THE MATRIX!------**

I'm just keeping the old working codes for a record.

```{r}
library(dplyr)
library(tidyr)

# 1) Count raw frequencies per (language, correct_response, response)
conf_by_lang <- big_df %>%
  count(language, correct_response, response, name = "freq")

# 2) (Optional) Add a proportion column within each language & true emotion
conf_by_lang <- conf_by_lang %>%
  group_by(language, correct_response) %>%
  mutate(
    prop = freq / sum(freq)
  ) %>%
  ungroup()

# 3) Pivot to wide format so you get a matrix of (true emotion × chosen emotion) for each language
#    – Rows: correct_response (true)
#    – Cols: response (chosen)
#    – Cell: either freq or prop
conf_mat_list <- conf_by_lang %>%
  select(language, correct_response, response, freq) %>%
  pivot_wider(
    names_from  = response,
    values_from = freq,
    values_fill = 0
  ) %>%
  group_by(language) %>%
  group_split()  # returns a list of data frames, one per language

# Now conf_mat_list[[i]] is a matrix‐like data frame for the i-th language,
# where the first column is 'language', second is 'correct_response', and
# the remaining columns are counts for each possible response label.

```

Confusion matrix -nicer-

```{r}
# 0) Install/load necessary packages
#install.packages(c("knitr", "kableExtra", "purrr"))
library(dplyr)
library(tidyr)
library(purrr)
library(knitr)
library(kableExtra)

# Assume you already have `conf_mat_list` as a list of data frames, one per language.
# Each element has columns: language, correct_response, and one column per possible response.

# For example, conf_mat_list[[1]] might look like:
#   language correct_response   angry  fear  happy  neutral  sad  surprise
#   1   French            angry     50    2     3        1    4        0
#   2   French             fear     1    47     5        2    3        2
#   …


# 1) Prepare a helper that formats one confusion‐matrix DF into a kable
format_conf_table <- function(cm_df, format = "latex", caption = NULL) {
  # Remove the 'language' column so rows are just correct_response × responses
  cm <- cm_df %>% select(-language)
  
  # Optionally add a total‐column or row if you’d like:
  # cm <- cm %>%
  #   mutate(RowTotal = rowSums(across(-correct_response))) %>%
  #   bind_rows(
  #     tibble(
  #       correct_response = "ColumnTotal",
  #       map_dbl(across(-correct_response), sum)
  #     )
  #   )
  
  # Turn it into a kable
  kbl(cm,
      format = format,
      caption = caption,
      booktabs = TRUE,
      align = c("l", rep("r", ncol(cm) - 1))
  ) %>%
    kable_styling(
      full_width = FALSE,
      position   = "center",
      font_size  = 10
    ) %>%
    add_header_above(c(" " = 1, "Predicted Label" = ncol(cm) - 1))
}

# 2) Loop through conf_mat_list and print each table (or save to file)

# If you want to *print* them all one after another (e.g., in an RMarkdown .Rmd):
walk(
  conf_mat_list,
  ~ {
    lang_name <- unique(.x$language)  # each df has a single language
    cap       <- paste0("Confusion matrix (", lang_name, ")")
    
    # Print LaTeX (or "html", "markdown", etc.)
    print(format_conf_table(.x, format = "latex", caption = cap))
    cat("\n\n")  # add some spacing between tables
  }
)



```

```{r}
# 3) If instead you want to *save* each as a separate .tex or .html file:
walk(
  conf_mat_list,
  ~ {
    lang_name <- unique(.x$language)
    cap       <- paste0("Confusion matrix (", lang_name, ")")
    tbl       <- format_conf_table(.x, format = "latex", caption = cap)
    
    # Write to a .tex file:
    out_path <- paste0("confusion_matrix_", lang_name, ".tex")
    cat(as.character(tbl), file = out_path)
  }
)
```

```{r}
# Load necessary packages (install if you haven’t already)
# install.packages(c("dplyr", "tidyr", "knitr", "kableExtra"))
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# ------------------------------------------------------------------------------
# Assume `conf_mat_list` already exists in your environment. Recall that
#   conf_mat_list is a list of data frames, one per language, each with columns:
#     • language           – (repeated) the language name
#     • correct_response   – the “true” emotion
#     • <response_1>, …     – counts for each chosen emotion label
#
# Below are two ways to turn `conf_mat_list` into nicely formatted tables.
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# OPTION 1: Produce a separate, labeled HTML/Markdown table for each language
# ------------------------------------------------------------------------------

for (idx in seq_along(conf_mat_list)) {
  # 1. Pull out the i-th confusion‐matrix data frame
  cm_df <- conf_mat_list[[idx]] %>%
    select(-language)        # drop the “language” column, since we’ll title by language
  
  # 2. Extract the language name (same on every row)
  this_lang <- conf_mat_list[[idx]]$language[1]
  
  # 3. Print a header, then the table via kable + kableExtra
  cat("\n\n")
  cat("## Confusion Matrix for:", this_lang, "\n\n")
  cm_df %>%
    kable(
      format    = "html", 
      caption   = paste0("Confusion Matrix (counts) — ", this_lang),
      digits    = 0, 
      align     = "c"
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width        = FALSE,
      position          = "center"
    )
}

# ------------------------------------------------------------------------------
# OPTION 2: Combine all languages into one “long” table, with a language column
# ------------------------------------------------------------------------------

# 1) Bind all rows together into a single data frame
combined_cm <- bind_rows(conf_mat_list)

# 2) Print as one big table (language + true emotion + counts for each response)
combined_cm %>%
  kable(
    format    = "html",
    caption   = "Confusion Matrices by Language (counts)",
    digits    = 0,
    align     = "c"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width        = FALSE,
    position          = "center"
  )

```

printing as a file

```{r}
# 1) Install/load required packages (if not already)
# install.packages("gridExtra")
library(gridExtra)
library(ggplot2)

# 2) Turn your data frame into a table “grob”
tg <- tableGrob(combined_cm)

# 3) Save that grob as a PNG file
#    – Adjust width/height (in inches) and dpi as needed.
ggsave(
  filename = "combined_confusion_matrix.png",
  plot     = tg,
  width    = 12,    # inches (you can change this)
  height   = 8,     # inches (you can change this)
  dpi      = 300    # 300 dpi is good for publication
)

```

printing with colored cells

```{r}
# 1) Load required packages (install if needed)
# install.packages(c("gridExtra", "grid", "dplyr"))
library(gridExtra)
library(grid)
library(dplyr)

# --- Assume `combined_cm` already exists in your workspace from previous steps ---
# combined_cm should look like this:
#   language | correct_response | happy | sad | angry | fear | surprise | neutral
#   ---------------------------------------------------------------------------
#   French   |     happy        |  15   |  2  |   0   |  1   |    0     |   2
#   Greek    |     sad          |   1   | 10  |   0   |  3   |    2     |   4
#   ... etc.


# 2) Define a palette‐making function for each language
pal_red    <- colorRampPalette(c("white", "red"))
pal_green  <- colorRampPalette(c("white", "green"))
pal_blue   <- colorRampPalette(c("white", "blue"))
pal_purple <- colorRampPalette(c("white", "purple"))

# 3) Build a matrix of background colors for every “body” cell
#
#    - n_row = number of rows in combined_cm
#    - n_col = number of columns in combined_cm
n_row <- nrow(combined_cm)
n_col <- ncol(combined_cm)

# 3a) Identify which columns are numeric (i.e. the confusion counts).
#     We assume the first two columns are “language” and “correct_response,” 
#     and everything else (3:n_col) are numeric counts.
numeric_cols <- 3:n_col

# 3b) Preallocate a character matrix of “fill” colors, same dimension as combined_cm
bg_mat <- matrix(NA, nrow = n_row, ncol = n_col)

# 3c) Loop over each row and column:
for (i in seq_len(n_row)) {
  # 1. Grab this row’s language
  lang <- combined_cm$language[i]
  
  # 2. Extract the numeric counts for this row
  vals <- as.numeric(combined_cm[i, numeric_cols])
  
  # 3. Normalize within row (max → 1, min → 0)
  #    If a row is all zeros (edge case), set all norms to 0 to avoid division-by-zero.
  if (all(vals == 0)) {
    row_norm <- rep(0, length(vals))
  } else {
    row_norm <- vals / max(vals)
  }
  
  # 4. Choose the appropriate palette function based on language:
  pal_fun <- switch(
    lang,
    "French"   = pal_red,
    "Greek"    = pal_green,
    "Japanese" = pal_blue,
    "Thai"     = pal_purple,
    # (If you have extra languages, add them here. Default → gray)
    pal_default <- colorRampPalette(c("white", "gray"))
  )
  
  # 5. Generate 100 shades from “white” → hue, then pick by normalized value:
  shades <- pal_fun(100)  # a vector of 100 hex colors from white to that hue
  #    For each normalized v in [0,1], pick shade index = floor(v*99) + 1.
  cell_colors <- shades[floor(row_norm * 99) + 1]
  
  # 6. Fill in bg_mat for numeric columns
  bg_mat[i, numeric_cols] <- cell_colors
  
  # 7. For the first two (non‐numeric) columns, leave as white (or NA)
  bg_mat[i, 1:2] <- "white"
}

# 4) Create a custom theme that uses bg_mat for the “core” cells
ttheme_colored <- ttheme_minimal(
  core = list(
    fg_params = list(col = "black", fontsize = 10), 
    bg_params = list(fill = bg_mat, col = NA)
  ),
  colhead = list(
    fg_params = list(col = "white", fontsize = 11, fontface = "bold"),
    bg_params = list(fill = "black", col = NA)
  ),
  rowhead = list(  # (not used, but define so that row names don't appear oddly)
    fg_params = list(col = "black", fontsize = 10),
    bg_params = list(fill = NA, col = NA)
  )
)

# 5) Turn combined_cm into a tableGrob with that theme
tbl <- tableGrob(
  combined_cm,
  theme = ttheme_colored,
  rows  = NULL  # hide rownames so we only see our two columns + numeric columns
)

# 6) Finally, save this grob as a PNG (adjust width/height as needed)
ggsave(
  filename = "combined_confusion_matrix_colored.png",
  plot     = tbl,
  width    = 12,    # inches—tweak to fit all columns legibly
  height   = 8,     # inches
  dpi      = 300
)

# ------------------------------------------------------------------------------
# After running the above, you’ll have a file named:
#   “combined_confusion_matrix_colored.png”
# in your working directory. It will show:
#  • For each language‐row, cells with higher counts are darker shades of that
#    language’s color (red/green/blue/purple).
#  • The two leftmost columns (“language” & “correct_response”) remain white 
#    with black text, so your numeric cells stand out in their color gradient.
# ------------------------------------------------------------------------------



```

The above outputs gray scaled.

**attempt 2:**

```{r}
# ──────────────────────────────────────────────────────────────────────────────
# 1. Load necessary packages (install if needed)
# ──────────────────────────────────────────────────────────────────────────────
# install.packages(c("gridExtra", "grid", "dplyr"))
library(dplyr)
library(gridExtra)
library(grid)

# ──────────────────────────────────────────────────────────────────────────────
# 2. Identify which columns are numeric (the count columns)
# ──────────────────────────────────────────────────────────────────────────────
# combined_cm has columns: language, correct_response, <response1>, <response2>, …
# We want to color only the “<response*>” columns.
numeric_cols <- setdiff(names(combined_cm), c("language", "correct_response"))
num_idx      <- match(numeric_cols, names(combined_cm))  # column indices of numeric columns

# ──────────────────────────────────────────────────────────────────────────────
# 3. Build a fill‐color matrix (rows × columns), NA where no fill is needed
# ──────────────────────────────────────────────────────────────────────────────
n_rows <- nrow(combined_cm)
n_cols <- ncol(combined_cm)

# Initialize every cell as NA (transparent)
fill_matrix <- matrix(NA_character_, nrow = n_rows, ncol = n_cols)

# Function to get a hex color for a normalized value using a color ramp
get_row_colors <- function(vals, ramp_func) {
  # vals: numeric vector (≥ 0). ramp_func: function(x) → c(r,g,b) (0–255)
  if (all(vals == 0)) {
    # If the entire row is zero, return all the lightest shade (white)
    rep("#FFFFFF", length(vals))
  } else {
    row_max   <- max(vals, na.rm = TRUE)
    norm_vals <- vals / row_max
    sapply(norm_vals, function(nv) {
      rgb_vals <- ramp_func(nv) / 255
      rgb(rgb_vals[1], rgb_vals[2], rgb_vals[3])
    })
  }
}

# Build the matrix row‐by‐row
for (i in seq_len(n_rows)) {
  lang     <- combined_cm$language[i]
  row_vals <- as.numeric(combined_cm[i, numeric_cols])
  
  # Choose ramp based on language
  if (lang == "french") {
    ramp <- colorRamp(c("white", "#F8766D"))
  } else if (lang == "greek") {
    ramp <- colorRamp(c("white", "#7CAE00"))
  } else if (lang == "japanese") {
    ramp <- colorRamp(c("white", "#00BFC4"))
  } else if (lang %in% c("thai", "Thai")) {
    ramp <- colorRamp(c("white", "#C77CFF"))
  } else {
    # Fallback to gray if language name is unexpected
    ramp <- colorRamp(c("white", "grey"))
  }
  
  # Compute a vector of hex colors for this row’s numeric columns
  this_row_colors <- get_row_colors(row_vals, ramp)
  
  # Place them into the appropriate columns of fill_matrix
  fill_matrix[i, num_idx] <- this_row_colors
}

# ──────────────────────────────────────────────────────────────────────────────
# 4. Create a theme that uses fill_matrix for cell backgrounds, then draw/save
# ──────────────────────────────────────────────────────────────────────────────
tt <- ttheme_default(
  core = list(
    fg_params = list(fontface = "plain", fontsize = 10),
    bg_params = list(fill = fill_matrix, col = NA)
  ),
  colhead = list(
    fg_params = list(fontface = "bold", fontsize = 11),
    bg_params = list(fill = "#DDDDDD", col = NA)
  )
)

# Build the table grob (drop default rownames)
tg <- tableGrob(combined_cm, rows = NULL, theme = tt)

# Save as PNG
png(
  filename = "colored_confusion_matrix2.png",
  width    = 12,    # inches
  height   = 8,     # inches
  units    = "in",
  res      = 300    # dpi
)
grid.draw(tg)
dev.off()

```

##################GGPLOT DEFAULT COLORS##################### \#

"#F8766D" "#7CAE00" "#00BFC4" "#C77CFF"

\###########################################################

**Now, percentage version:**

**THIS IS THE FINAL VER!!!!!!!!!!! (TO RECREATE, just need to run the 2 chunks below)**\

```{r}
#############"precondition" stuff copied here"###############

library(dplyr)
library(tidyr)

# 1) Count raw frequencies per (language, correct_response, response)
conf_by_lang <- big_df %>%
  count(language, correct_response, response, name = "freq")

# 2) (Optional) Add a proportion column within each language & true emotion
conf_by_lang <- conf_by_lang %>%
  group_by(language, correct_response) %>%
  mutate(
    prop = freq / sum(freq)
  ) %>%
  ungroup()

# 3) Pivot to wide format so you get a matrix of (true emotion × chosen emotion) for each language
#    – Rows: correct_response (true)
#    – Cols: response (chosen)
#    – Cell: either freq or prop
conf_mat_list <- conf_by_lang %>%
  select(language, correct_response, response, freq) %>%
  pivot_wider(
    names_from  = response,
    values_from = freq,
    values_fill = 0
  ) %>%
  group_by(language) %>%
  group_split()  # returns a list of data frames, one per language

# Now conf_mat_list[[i]] is a matrix‐like data frame for the i-th language,
# where the first column is 'language', second is 'correct_response', and
# the remaining columns are counts for each possible response label.

# 1) Bind all rows together into a single data frame
combined_cm <- bind_rows(conf_mat_list)

```

```{r}
# ──────────────────────────────────────────────────────────────────────────────
# Precondition: `combined_cm` exists with columns:
#   • language         (character)
#   • correct_response (character)
#   • <response1>, <response2>, …  (numeric counts)
#
# Goal: Display percentages per row (including zeros) and color cells by row‐wise percentage.
# ──────────────────────────────────────────────────────────────────────────────

# 1. Load required packages
# install.packages(c("dplyr", "gridExtra", "grid"))
library(dplyr)
library(gridExtra)
library(grid)

# 2. Identify numeric‐count columns and their indices
numeric_cols <- setdiff(names(combined_cm), c("language", "correct_response"))
num_idx      <- match(numeric_cols, names(combined_cm))

# 3. Compute a matrix of row‐wise percentages (0–1) and a display matrix of formatted strings
n_rows <- nrow(combined_cm)
n_cols <- ncol(combined_cm)

# Initialize matrices
perc_matrix <- matrix(0, nrow = n_rows, ncol = length(numeric_cols))
disp_matrix <- matrix("", nrow = n_rows, ncol = length(numeric_cols))

for (i in seq_len(n_rows)) {
  # Extract raw counts for this row
  counts    <- as.numeric(combined_cm[i, numeric_cols])
  row_total <- sum(counts, na.rm = TRUE)
  
  if (row_total > 0) {
    row_perc <- counts / row_total    # proportions between 0 and 1
  } else {
    row_perc <- rep(0, length(counts)) # if total is zero, keep zeros
  }
  
  perc_matrix[i, ] <- row_perc
  
  # Format as percentage strings (e.g., "25.0%")
  disp_matrix[i, ] <- sprintf("%.1f", row_perc * 100)
}

# 4. Build a display data frame (`display_df`) that has:
#    • language
#    • correct_response
#    • the percent strings in place of raw counts
#
display_df <- combined_cm %>%
  # Start with language & correct_response
  select(language, correct_response) %>%
  # Then bind the character matrix of percentages (with appropriate column names)
  bind_cols(as.data.frame(disp_matrix, stringsAsFactors = FALSE))

# Assign the same column names to the percent columns
colnames(display_df)[3:ncol(display_df)] <- numeric_cols

# 5. Build a fill‐color matrix using `perc_matrix`
#    (so higher % → darker color). We’ll use white→color ramps per language.
fill_matrix <- matrix(NA_character_, nrow = n_rows, ncol = n_cols)

# Helper to convert a numeric vector [0,1] to hex colors via a ramp function
get_row_colors <- function(norm_vals, ramp_func) {
  # norm_vals: numeric vector between 0 and 1
  if (all(norm_vals == 0)) {
    rep("#FFFFFF", length(norm_vals))
  } else {
    sapply(norm_vals, function(v) {
      rgb_vals <- ramp_func(v) / 255
      rgb(rgb_vals[1], rgb_vals[2], rgb_vals[3])
    })
  }
}

for (i in seq_len(n_rows)) {
  lang     <- combined_cm$language[i]
  norm_vals <- perc_matrix[i, ]
  
  # Select color ramp by lowercase language
  lang_lc <- tolower(lang)
  if (lang_lc == "french") {
    ramp <- colorRamp(c("white", "#F8766D"))
  } else if (lang_lc == "greek") {
    ramp <- colorRamp(c("white", "#7CAE00"))
  } else if (lang_lc == "japanese") {
    ramp <- colorRamp(c("white", "#00BFC4"))
  } else if (lang_lc == "thai") {
    ramp <- colorRamp(c("white", "#C77CFF"))
  } else {
    ramp <- colorRamp(c("white", "grey"))
  }
  
  this_row_colors <- get_row_colors(norm_vals, ramp)
  fill_matrix[i, num_idx] <- this_row_colors
}

# 6. Create a table theme that uses `fill_matrix` for the cell backgrounds,
#    and draw the table using `display_df` (with percent strings).
tt <- ttheme_default(
  core = list(
    fg_params = list(fontface = "plain", fontsize = 10),
    bg_params = list(fill = fill_matrix, col = NA)
  ),
  colhead = list(
    fg_params = list(fontface = "bold", fontsize = 11),
    bg_params = list(fill = "#DDDDDD", col = NA)
  )
)

# Build the table grob (drop rownames)
tg <- tableGrob(display_df, rows = NULL, theme = tt)

# 7. Save as PNG
png(
  filename = "colored_confusion_matrix_percent.png",
  width    = 12,    # inches
  height   = 8,     # inches
  units    = "in",
  res      = 300    # dpi
)
grid.draw(tg)

```

## Statistical analysis

to do:

-   overall accuracy - chance level?

-   base model - correct \~ 1 + random effects

-   showing overall effect of language, and overall effect of emotion.

-   if converge, post hoc.

-   planB: if no converge, look only at angry, sad, happy

### Df prep

```{r}
library(lme4)
library(dplyr)

big_df <- big_df %>%
  mutate(
    
    language         = factor(language),
    correct_response = factor(correct_response),

    ProlID    = factor(ProlID),
    speakerID = factor(speakerID),
    sentenceID= factor(sentenceID),
    
    correct = as.integer(correct)
  )
```

```{r}
nrow(demo_filtered)

```

Why do i have 103 data? i think i should only have 102...

### Data drop pt2

demo info prob has the participants info whose file was not saved on osf...

```{r}
library(dplyr)
cleaned_surveyr_17Pdropped <- subset(
  cleaned_surveyr,
  ! ProlID %in% datatodrop
)
```

This worked, and became 102!! this is the list of IDs with data.

```{r}

drop_ids <- str_extract(
  datatodrop,
  '(?<="prolific_id":")[^"]+'
)

demo_filtered<-subset(demo_filtered,! Participant.id %in% drop_ids)
```

This is the list of IDs from Prolific's demographic data. 103 total.

```{r}
idlist_of_cleaned_surveyr_17Pdropped <- str_extract(
  cleaned_surveyr_17Pdropped$ProlID,
  '(?<="prolific_id":")[^"]+'
)

########-------------------------########

# idlist_of_cleaned_surveyr_17Pdropped
#     this is from self report at the beginning of the experiment
#
# demo_filtered$Participant.id
#     this is directly from Prolific

########-------------------------#########

missing_in_demo <- setdiff(
  idlist_of_cleaned_surveyr_17Pdropped,
  demo_filtered$Participant.id
)

missing_in_survey <- setdiff(
  demo_filtered$Participant.id,
  idlist_of_cleaned_surveyr_17Pdropped
)

missing_in_demo #this is what is NOT in Prolific data, but selfreported
missing_in_survey #this is what is NOT in self reported, but in Prolific data
```

Now demo info should be cut down in the same way (aka drop the 17 from demo_filtered) –\> and then combine! then rerun descriptive with only the kept data.

```{r}

files <- list.files(
  path       = "C:/Users/yuka/Desktop/linguist245B_project/data/osfstorage-archive",
  pattern    = "\\.csv$",
  full.names = TRUE
)

string_to_find <- "666f76b68b89442817be678a"
found_files   <- character()  # will collect any matching file paths

for (fpath in files) {
  # read with stringsAsFactors = FALSE to keep everything as character
  data <- read.csv(fpath, header = TRUE, sep = ",", stringsAsFactors = FALSE)
  
  # flatten to a character matrix and test for the email
  if (any(grepl(string_to_find, as.matrix(data), fixed = TRUE), na.rm = TRUE)) {
    found_files <- c(found_files, fpath)
  }
}

found_files

```

Self reported ('copy and pasted' at the beginning of the study), but NOT in Prolific data:

-   graniaheadwardshemphill254\@gmail.com ... 468o13zzu8.csv...osf says 3:46 saved this data

-   657f8d99678be2d216a85c3d ... on demo df, this is marked as RETURNED. Apparently this person completed but returned. (possibly bc they realized their data shouldnt be used?) I should delete this data too.

Found in demo data, but NOT self reported: (all these ppl are marked approrved. one should be the gmail person, another is the person returned, the other is the one OSF failed to save?)...

-   666f76b68b89442817be678a ... time taken 789 sec (13.15min) ...closest to sko3w425dv.csv's time elapsed, only 0.8sec off. this file reported "67e28d5b36505c655badd4b0" as their ID. but this ID does exist in demo df so not this...time taken 828

    -   ended at: 2025-05-30T06:41:15.306000Z--- so 3:41 ish

-   6834d360dff6bfa3d5c6dca8 ... according to demo df, time taken = 1080 sec (18min); 17:59, according to the Prolific portal

    -   ended at 6:47am UTC so 3:47pm JPT=11:47pmPDT

    -   demo df: ended at: 2025-05-30T06:47:03.782000Z (UTC)

-   67c8d564424823af9a9c3293 ... according to demo df, time taken = 1057 sec (17.61667min); 17:36, according to the Prolific portal.

    -   ended at 7:14am UTC so 4:14pm JPT = 0:14am PDT

    -   demo df: ended at: 2025-05-30T07:14:39.121000Z (UTC)

Basically all these mean: I thought there are 102 valid data files after cutting 17 data out, but there are actually 101 (the one person returned after compeletion). Out of 101, 1 of them cannot be linked to the demographic data (they put their gmail). There are 3 demographics data recorded and they are approved, but no data exist for 2 of them and 1 of them is probably the gmail person. Out of the rest 2, one is the person whose data is missing on osf, which makes sense cus i only got 119 data overall. I have no idea what happened to the other one data. According to the OSF record, 683 ended at 3:47, while 67c ended at 4:14.

For demographics data, I just want to identify which of the 3 belongs to the gmail person. The only information that can connect these two are the length of the experiment. 468o13zzu8.csv suggests that 16.58min has passed when they saw the score. Out of the 3 demo data, No2 (data starting with 683) and No3 (data starting with 67c) are longer than that, so not the 666 data. But both 683 and 67c are about 18min, not really sure which one.

Now, osf says 3:46 was when 468o13zzu8.csv was recorded. This is in JPT somehow, so that is equivalent to 6:46am UTC. 683 was saved at 6:47am, while 67c was saved at 7:14am. This strongly suggests that 683 is the demo data for 468o13zzu8.csv person.

Below is the code to 1)cut data 657f8d99678be2d216a85c3d and make total 101, and 2) replace gmail with the real Prolific ID.

```{r}
#no more data used from 657f8d99678be2d216a85c3d because this person retruned.
big_df <- big_df %>%
  filter(ProlID != '{"prolific_id":"657f8d99678be2d216a85c3d"}')

```

```{r}
#replacing gmail data.(if this is problematic i can delete this too later)
big_df <- big_df %>%
  mutate(
    ProlID = if_else(
      ProlID == '{"prolific_id":"graniaheadwardshemphill254@gmail.com"}',
      '{"prolific_id":"6834d360dff6bfa3d5c6dca8"}',
      ProlID
    )
  )
```

```{r}
#add 657f8d99678be2d216a85c3d to this for future use of datatodrop.
datatodrop <- append(datatodrop,"{\"prolific_id\":\"657f8d99678be2d216a85c3d\"}")
```

```{r}
#drop also from cleaned_surveyr_17Pdropped
cleaned_surveyr_18Pdropped<- cleaned_surveyr_17Pdropped  %>%
  filter(ProlID != '{"prolific_id":"657f8d99678be2d216a85c3d"}')
#replacing gmail data for this too
cleaned_surveyr_18Pdropped <- cleaned_surveyr_18Pdropped %>%
  mutate(
    ProlID = if_else(
      ProlID == '{"prolific_id":"graniaheadwardshemphill254@gmail.com"}',
      '{"prolific_id":"6834d360dff6bfa3d5c6dca8"}',
      ProlID
    )
  )
```

```{r}
#drop also from demo_filtered
demo_filtered<- demo_filtered  %>%
  filter(Participant.id != "657f8d99678be2d216a85c3d")
#replacing gmail data for this too
demo_filtered <- demo_filtered %>%
  mutate(
    Participant.id = if_else(
      Participant.id == "graniaheadwardshemphill254@gmail.com",
      "6834d360dff6bfa3d5c6dca8",
      Participant.id
    )
  )
```

Data dropped and updated for

-   big_df (main data)

-   cleaned_surveyr_18Pdropped (survey info, newly made)

-   demo_filtered (demograpahic data)

### Descriptive

accuracy overall

```{r}
library(dplyr)
library(ggplot2)

participant_acc <- big_df %>%
  group_by(ProlID) %>%
  summarize(accuracy = mean(correct), .groups = "drop")

mean_accuracy_across_participants <- mean(participant_acc$accuracy)
print(mean_accuracy_across_participants)

```

Accuracy by overall emotion and language

```{r}
emotion_avg_raw <- big_df %>%
  group_by(correct_response) %>%
  summarize(mean_accuracy = mean(correct), .groups = "drop")

print(emotion_avg_raw)

language_avg_raw <- big_df %>%
  group_by(language) %>%
  summarize(mean_accuracy = mean(correct), .groups = "drop")

print(language_avg_raw)


```

Quick Visualization: (kinda meaningless)

```{r}
acc_lang_emotion <- big_df %>%
  group_by(ProlID, language, correct_response) %>%
  summarize(accuracy = mean(correct), .groups = "drop")

ggplot(acc_lang_emotion, aes(x = language, y = accuracy, color = correct_response)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.7, size = 2) +
  stat_summary(
    aes(group = correct_response),
    fun = mean,
    geom = "point",
    shape = 18,
    size = 3,
    color = "black"
  ) +
  facet_wrap(~ correct_response, nrow = 1) +
  labs(
    x = "Language",
    y = "Accuracy (proportion correct)",
    color = "Emotion",
    title = "Participant-wise Accuracy by Language and Emotion",
    subtitle = paste0("Overall mean across participants: ", round(mean_accuracy_across_participants, 3))
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

### Demographics

Summary

```{r}
library(dplyr)
descriptives <- list(
  numeric = demo_filtered %>%
    summarise(
      n         = n(),
      mean_time = mean(Time.taken, na.rm = TRUE),
      sd_time   = sd(Time.taken,   na.rm = TRUE),
      mean_age  = mean(Age,        na.rm = TRUE),
      sd_age    = sd(Age,          na.rm = TRUE)
    ),
  sex           = demo_filtered %>% count(Sex)           %>% mutate(percent = round(n / sum(n) * 100, 1)),
  ethnicity     = demo_filtered %>% count(Ethnicity.simplified) %>% mutate(percent = round(n / sum(n) * 100, 1)),
  student_stat  = demo_filtered %>% count(Student.status) %>% mutate(percent = round(n / sum(n) * 100, 1)),
  employment    = demo_filtered %>% count(Employment.status) %>% mutate(percent = round(n / sum(n) * 100, 1))
)

```

Visualizations

```{r}
library(dplyr)
library(ggplot2)

# helper that takes an unquoted column name
create_pie <- function(data, var, title) {
  df <- data %>%
    count({{var}}) %>%
    rename(group = {{var}}, count = n) %>%
    mutate(pct = count / sum(count) * 100)

  ggplot(df, aes(x = "", y = pct, fill = group)) +
    geom_col(width = 1) +
    coord_polar(theta = "y") +
    geom_text(aes(label = ifelse(pct >= 5, paste0(round(pct), "%"), "")),
              position = position_stack(vjust = 0.5)) +
    labs(title = title, fill = NULL) +
    theme_void() +
    scale_fill_brewer(palette = "Pastel2")
}


```

```{r}
#Compute raw counts
sex_counts        <- demo_filtered %>% count(Sex) %>% arrange(desc(n))
ethnicity_counts  <- demo_filtered %>% count(Ethnicity.simplified) %>% arrange(desc(n))
student_counts    <- demo_filtered %>% count(Student.status) %>% arrange(desc(n))
employment_counts <- demo_filtered %>% count(Employment.status) %>% arrange(desc(n))

```

```{r}
#tables
kable(sex_counts, col.names = c("Sex", "Count"), caption = "Sex") %>%
  kable_styling(position = "center", full_width = FALSE) 

kable(ethnicity_counts, col.names = c("Ethnicity", "Count"), caption = "Ethnicity") %>% kable_styling(position = "center", full_width = FALSE) 

kable(student_counts, col.names = c("Student Status", "Count"), caption = "Student Status") %>%
  kable_styling(position = "center", full_width = FALSE) 

kable(employment_counts, col.names = c("Employment Status", "Count"), caption = "Employment Status") %>%
  kable_styling(position = "center", full_width = FALSE) 

```

```{r}
create_pie(demo_filtered, Sex, "Sex Distribution")


```

```{r}
ggsave("sex_distribution_pie_chart.png")
```

```{r}

create_pie(demo_filtered,Ethnicity.simplified, "Ethnicity Distribution")


```

```{r}
ggsave("ethnicity_pie_chart.png")

```

```{r}
create_pie(demo_filtered,Student.status,   "Student-Status Distribution")


```

```{r}
ggsave("student_status_pie_chart.png")
```

```{r}
create_pie(demo_filtered,Employment.status, "Employment Status Distribution")
```

```{r}
#saves the last ran chart
ggsave(
  filename = "employment_status_pie_chart.png"
)
```

### Inferential 

#### base model

```{r}
model_base <- glmer(correct ~ 1 + (1 | ProlID) + (1| speakerID) + 
                      (1 | sentenceID),data =  big_df,family = binomial)
summary(model_base)
```

#### no interaction, just language effect model

```{r}
model_noint <- glmer(
  correct ~ language +
    (1 | ProlID) +
    (1 | speakerID) +
    (1 | sentenceID),
  data   = big_df,
  family = binomial,
)

summary(model_noint)
```

contrast language model and base model

```{r}
anova(model_base, model_noint) 
```

significcantly different –\> languages have effect!

#### no interaction, by only emotion model

```{r}
model_noint2 <- glmer(
  correct ~ correct_response +
    (1 | ProlID) +
    (1 | speakerID) +
    (1 | sentenceID),
  data   = big_df,
  family = binomial,
)

summary(model_noint2)
```

```{r}
anova(model_base, model_noint2)
```

sig diff –\> evidence for emotion influence to performance!

#### no interaction, both main predictors

```{r}
model_noint3 <- glmer(
  correct ~ language + correct_response +
    (1 | ProlID) + (1 | speakerID) + (1 | sentenceID) ,
  data   = big_df,
  family = binomial,
control=glmerControl(optimizer="bobyqa"))

summary(model_noint3)

```

#note: added optimizer. language + correct_response + (1 \| ProlID) + (1 + correct_response \| speakerID) was singular fit. Then, simplified to prolID/speakerID/sentenceID, which worked (and this is the same structure as the other previous models!)

```{r}
anova(model_base, model_noint, model_noint2, model_noint3)
```

Summary so far

-   model_base: Only random effects

-   model_noint: Only language

-   model_noint2: Only correct_response

-   model_noint3: both language and correct_response

The best model is model_noint3, since both predictors (language and correct_response) jointly explain more variance than either alone.

The improvement from model_noint2 to model_noint3 is statistically significant (p = 0.0084), though not nearly as dramatic as from model_base to model_noint2.

So,

-   correct_response explains a huge amount.

-   language explains a bit more even after accounting for that.

In other words:

-   model_noint improves over model_base → language alone helps

-    model_noint2 improves over model_base → correct_response alone helps a lot

-   model_noint3 improves over model_noint2 → language still adds significant incremental value beyond correct_response

#### Interaction model

```{r}

model <- glmer(
  correct ~ language * correct_response + 
    (1 | ProlID) + (1 | speakerID) + (1 | sentenceID),
  data   = big_df,
  family = binomial,
control=glmerControl(optimizer="bobyqa"))

summary(model)

```

comparison

```{r}
anova(model, model_noint3)
```

This suggests overall interaction model is a lot better than the no-interaction model!

#### Seeing distributions of items across conditions

```{r}
#just to see and confirm data distributions across conditions
table(big_df$language, big_df$correct_response)
```

#### Post-hoc

every condition

```{r}

library(emmeans)

emm <- emmeans(
  object   = model,
  specs    = ~ language * correct_response,
  type     = "response"
)


contrast_lang_within_emotion <- emm %>%
  contrast(
    method = "pairwise",              # all‐pair comparisons
    by     = "correct_response",      # do them separately for each emotion
    adjust = "tukey"                  # Tukey adjustment
  )

contrast_emotion_within_language <- emm %>%
  contrast(
    method = "pairwise",
    by     = "language",
    adjust = "tukey"
  )

summary(emm)
summary(contrast_lang_within_emotion, infer = TRUE)
summary(contrast_emotion_within_language, infer = TRUE)

```

Post hoc, for LANGUAGE predictor

```{r}
library(emmeans)

emm_lang <- emmeans(model_noint3, ~ language, type = "correct")

summary(emm_lang)

pairs(emm_lang, adjust = "tukey")


```

###results

Only sig: French-greek and greek-japanese

Post hoc, for EMOTION predictor

```{r}
library(emmeans)

emm_emo <- emmeans(model_noint3, ~ correct_response, type = "correct")

summary(emm_emo)

pairs(emm_emo, adjust = "tukey")
```

mostly different except for fear-sad and happy-surprise

#### 3 emotion models

No 2-predictor models converged because of this issue (according to GPT): If any cell in that table has very few observations (especially zero failures or zero successes), the logistic likelihood will try to push its coefficient toward ±∞, causing nonconvergence.

Therefore trying models where only looking at the emotion tested across languages: happy, angry, sad

```{r}

library(dplyr)
threeemo_df <- big_df %>%
  filter(correct_response %in% c("Sad", "Angry", "Happy"))

```

```{r}
model_3emo_1r <- glmer(
  correct ~ language + correct_response +  (1 | ProlID),
  data   = threeemo_df,
  family = binomial)

summary(model_3emo_1r)

```

this one fits!!!! finally...

adding another

```{r}
model_3emo_2r <- glmer(
  correct ~ language + correct_response +  (1 | ProlID) +(1|speakerID),
  data   = threeemo_df,
  family = binomial)

summary(model_3emo_2r)

```

this fits too

```{r}
model_3emo_3r <- glmer(
  correct ~ language + correct_response +  (1 | ProlID) +(1|speakerID) +(1|sentenceID),
  data   = threeemo_df,
  family = binomial)

summary(model_3emo_3r)

```

This fits too

Trying interaction

Note: 3 rand no fit

```{r}

model_3emo_3r_int <- glmer(
  correct ~ language * correct_response + (1|ProlID),
  data   = threeemo_df,
  family = binomial,
control=glmerControl(optimizer="bobyqa"))

summary(model_3emo_3r_int)

```

This does not fit...

Trying another optimzer

```{r}
library(optimx)  # required for some optimizers

glmm_all = allFit(model_3emo_3r_int)
```

```{r}
anova(model_3emo_3r, model_3emo_3r_int)
```

To do:

-   add confidence interval –\> DONE

-   check attention check –\> DONE

-   look at demographics? age?, also other survey results

-   overtime, neutral probability goes up?

## Survey Responses—

end-of-the-experiment surveys

```{r}
# make sure these are loaded
library(dplyr)
library(purrr)
library(jsonlite)

# 1) Get all your files
files <- list.files(
  path       = "C:/Users/yuka/Desktop/linguist245B_project/data/osfstorage-archive",
  pattern    = "\\.csv$",
  full.names = TRUE
)

# 2) Process each file, extract choice trials + ProlID + survey answers
big_df_wSurvey <- files %>%
  map_df(function(fpath) {
    data    <- read.csv(fpath, header = TRUE, sep = ",")
    prolID  <- data[1, 2]    # your ID

    # find the survey row (skip row 1)
    survey_idx <- which(
      data$trial_type == "survey-html-form" &
      seq_len(nrow(data)) != 1
    )
    if (length(survey_idx) > 0) {
      survey_json <- data[survey_idx[1], 2]
      survey_info <- fromJSON(survey_json)
    } else {
      # in case there's no survey row
      survey_info <- list(
        language_guesses  = NA_character_,
        tech_difficulity  = NA_character_,
        distraction       = NA_character_,
        other_languages   = NA_character_,
        media_exposures   = NA_character_,
        family_exposure   = NA_character_,
        comments          = NA_character_
      )
    }

    data %>%
      filter(trial_part == "choice") %>%
      select(2, 14, 15, 16, 18) %>%        # your original columns
      add_column(ProlID = prolID, .before = 1) %>%
      mutate(
        language_guesses = survey_info$language_guesses,
        tech_difficulity = survey_info$tech_difficulity,
        distraction      = survey_info$distraction,
        other_languages  = survey_info$other_languages,
        media_exposures  = survey_info$media_exposures,
        family_exposure  = survey_info$family_exposure,
        comments         = survey_info$comments
      )
  })

# 3) Then your recoding step as before
big_df_wSurvey <- big_df_wSurvey %>%
  mutate(
    response = recode(
      response,
      `0` = "Happy",
      `1` = "Sad",
      `2` = "Angry",
      `3` = "Fear",
      `4` = "Surprise",
      `5` = "Neutral"
    ),
    correct = as.integer(as.logical(correct))
  )



```

ONLY ID and survey resp.

```{r}
library(dplyr)
library(purrr)
library(jsonlite)

# 1) Your file list
files <- list.files(
  path       = "C:/Users/yuka/Desktop/linguist245B_project/data/osfstorage-archive",
  pattern    = "\\.csv$",
  full.names = TRUE
)

# 2) Map over files to pull ProlID + survey info
survey_df <- files %>%
  map_df(function(fpath) {
    data   <- read.csv(fpath, header = TRUE, sep = ",")
    prolID <- data[1, 2]

    # find the first non‐header survey row
    survey_idx <- which(
      data$trial_type == "survey-html-form" &
      seq_len(nrow(data)) != 1
    )[1]

    if (!is.na(survey_idx)) {
      survey_json <- data[survey_idx, 2]
      survey_info <- fromJSON(survey_json)
    } else {
      # if missing, fill with NA
      survey_info <- list(
        language_guesses  = NA_character_,
        tech_difficulity  = NA_character_,
        distraction       = NA_character_,
        other_languages   = NA_character_,
        media_exposures   = NA_character_,
        family_exposure   = NA_character_,
        comments          = NA_character_
      )
    }

    # return a one‐row tibble
    tibble(
      ProlID           = prolID,
      language_guesses = survey_info$language_guesses,
      tech_difficulity = survey_info$tech_difficulity,
      distraction      = survey_info$distraction,
      other_languages  = survey_info$other_languages,
      media_exposures  = survey_info$media_exposures,
      family_exposure  = survey_info$family_exposure,
      comments         = survey_info$comments
    )
  })


```

output survey results to csv

```{r}
write.csv(survey_df,"C:/Users/yuka/Desktop/linguist245B_project/data/surveydata.csv", row.names = FALSE)
```

\~ I hand-cleaned the data \~

```{r}

cleaned_surveyr <- read.csv("C:/Users/yuka/Desktop/linguist245B_project/data/surveyresults_cleaned.csv", header = TRUE, sep = ",")

```

```{r}
View(cleaned_surveyr)
```

Visualization

```{r}

library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

```

```{r}


#Subset data
df_subset <- cleaned_surveyr %>%
  select(
    ProlID,
    cleaned_language_guesses,
    cleaned_tech_difficulity,
    cleaned_other_languages,
    cleaned_media_exposures,
    cleaned_family_exposure,
    media_exposure_to_stimuli_lang,
    family_exposure_to_stimuli_language
  )

#Count NAs in cleaned_language_guesses
na_count <- sum(
  is.na(df_subset$cleaned_language_guesses) |
  df_subset$cleaned_language_guesses == "NA" |
  df_subset$cleaned_language_guesses == ""
)
```

```{r}

#Tidy & tally, then order factor by descending count
lang_counts <- df_subset %>%
  mutate(
    cleaned_language_guesses = ifelse(
      is.na(cleaned_language_guesses) |
      cleaned_language_guesses == "NA" |
      cleaned_language_guesses == "",
      NA_character_,
      cleaned_language_guesses
    )
  ) %>%
  separate_rows(cleaned_language_guesses, sep = ",") %>%
  mutate(cleaned_language_guesses = str_trim(cleaned_language_guesses)) %>%
  filter(!is.na(cleaned_language_guesses)) %>%
  count(language = cleaned_language_guesses) %>%
  arrange(desc(n)) %>%
  mutate(
    language = factor(language, levels = language),
    label = paste0(language, " (", n, ")")
  )

#Plot with conditional labels
ggplot(lang_counts, aes(x = "", y = n, fill = language)) +
  geom_col(width = 1, color = "white") +
  coord_polar(theta = "y") +
  geom_text(
    aes(x = 1.3, label = ifelse(n > 3, label, NA_character_)),
    position = position_stack(vjust = 0.5),
    show.legend = FALSE
  ) +
  labs(
    title   = "Distribution of Language Guesses",
    fill    = "Language",
    caption = paste("No response (NA):", na_count)
  ) +
  theme_void()
```

Save

```{r}
# saves the last ggplot as a 6×6 in PNG at 300 dpi
ggsave(
  filename = "language_guess_pie_chart.png",
  width    = 6,
  height   = 6,
  units    = "in",
  dpi      = 300
)


```

## Making metadata for Hf data set creation—

```{r}
library(dplyr)

metadata_df <- big_df %>%
  select(
    stimulus_file,
    language,
    emotion       = correct_response,
    speakerID,
    sentenceID
  ) %>%
  distinct()



```

\-

```{r}

write.csv(big_df, "metadata.csv", row.names = FALSE)

```
